
@article{gentzkow_text_2019,
	title = {Text as {Data}},
	volume = {57},
	issn = {0022-0515},
	url = {https://pubs.aeaweb.org/doi/10.1257/jel.20181020},
	doi = {10.1257/jel.20181020},
	language = {en},
	number = {3},
	urldate = {2020-04-04},
	journal = {Journal of Economic Literature},
	author = {Gentzkow, Matthew and Kelly, Bryan and Taddy, Matt},
	month = sep,
	year = {2019},
	pages = {535--574},
}

@incollection{aref_women_2020,
	address = {Cham},
	title = {Women {Worry} {About} {Family}, {Men} {About} the {Economy}: {Gender} {Differences} in {Emotional} {Responses} to {COVID}-19},
	volume = {12467},
	isbn = {978-3-030-60974-0 978-3-030-60975-7},
	shorttitle = {Women {Worry} {About} {Family}, {Men} {About} the {Economy}},
	url = {http://link.springer.com/10.1007/978-3-030-60975-7_29},
	language = {en},
	urldate = {2020-10-25},
	booktitle = {Social {Informatics}},
	publisher = {Springer},
	author = {van der Vegt, Isabelle and Kleinberg, Bennett},
	editor = {Aref, Samin and Bontcheva, Kalina and Braghieri, Marco and Dignum, Frank and Giannotti, Fosca and Grisolia, Francesco and Pedreschi, Dino},
	year = {2020},
	doi = {10.1007/978-3-030-60975-7_29},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {397--409},
}

@inproceedings{kleinberg_measuring_2020,
	address = {Online},
	title = {Measuring {Emotions} in the {COVID}-19 {Real} {World} {Worry} {Dataset}},
	url = {https://www.aclweb.org/anthology/2020.nlpcovid19-acl.11},
	abstract = {The COVID-19 pandemic is having a dramatic impact on societies and economies around the world. With various measures of lockdowns and social distancing in place, it becomes important to understand emotional responses on a large scale. In this paper, we present the first ground truth dataset of emotional responses to COVID-19. We asked participants to indicate their emotions and express these in text. This resulted in the Real World Worry Dataset of 5,000 texts (2,500 short + 2,500 long texts). Our analyses suggest that emotional responses correlated with linguistic measures. Topic modeling further revealed that people in the UK worry about their family and the economic situation. Tweet-sized texts functioned as a call for solidarity, while longer texts shed light on worries and concerns. Using predictive modeling approaches, we were able to approximate the emotional responses of participants from text within 14\% of their actual value. We encourage others to use the dataset and improve how we can use automated methods to learn about emotional responses and worries about an urgent problem.},
	urldate = {2020-11-02},
	booktitle = {Proceedings of the 1st {Workshop} on {NLP} for {COVID}-19 at {ACL} 2020},
	publisher = {Association for Computational Linguistics},
	author = {Kleinberg, Bennett and van der Vegt, Isabelle and Mozes, Maximilian},
	month = jul,
	year = {2020},
}

@book{salganik_bit_2019,
	address = {Princeton, NJ},
	title = {Bit by bit: {Social} research in the digital age},
	isbn = {978-0-691-19610-7},
	shorttitle = {Bit by bit},
	language = {English},
	publisher = {Princeton University Press},
	author = {Salganik, Matthew J},
	year = {2019},
	note = {OCLC: 1134658838},
}

@article{boyd_natural_2021,
	title = {Natural {Language} {Analysis} and the {Psychology} of {Verbal} {Behavior}: {The} {Past}, {Present}, and {Future} {States} of the {Field}},
	volume = {40},
	issn = {0261-927X},
	shorttitle = {Natural {Language} {Analysis} and the {Psychology} of {Verbal} {Behavior}},
	url = {https://doi.org/10.1177/0261927X20967028},
	doi = {10.1177/0261927X20967028},
	abstract = {Throughout history, scholars and laypeople alike have believed that our words contain subtle clues about what we are like as people, psychologically speaking. However, the ways in which language has been used to infer psychological processes has seen dramatic shifts over time and, with modern computational technologies and digital data sources, we are on the verge of a massive revolution in language analysis research. In this article, we discuss the past and current states of research at the intersection of language analysis and psychology, summarizing the central successes and shortcomings of psychological text analysis to date. We additionally outline and discuss a critical need for language analysis practitioners in the social sciences to expand their view of verbal behavior. Lastly, we discuss the trajectory of interdisciplinary research on language and the challenges of integrating analysis methods across paradigms, recommending promising future directions for the field along the way.},
	language = {en},
	number = {1},
	urldate = {2021-08-18},
	journal = {Journal of Language and Social Psychology},
	author = {Boyd, Ryan L. and Schwartz, H. Andrew},
	month = jan,
	year = {2021},
	note = {Publisher: SAGE Publications Inc},
	keywords = {attention, computational social science, language analysis, natural language processing},
	pages = {21--41},
}

@article{mozes_repeated-measures_2021,
	title = {A repeated-measures study on emotional responses after a year in the pandemic},
	volume = {11},
	copyright = {2021 The Author(s)},
	issn = {2045-2322},
	url = {https://www.nature.com/articles/s41598-021-02414-9},
	doi = {10.1038/s41598-021-02414-9},
	abstract = {The introduction of COVID-19 lockdown measures and an outlook on return to normality are demanding societal changes. Among the most pressing questions is how individuals adjust to the pandemic. This paper examines the emotional responses to the pandemic in a repeated-measures design. Data (n = 1698) were collected in April 2020 (during strict lockdown measures) and in April 2021 (when vaccination programmes gained traction). We asked participants to report their emotions and express these in text data. Statistical tests revealed an average trend towards better adjustment to the pandemic. However, clustering analyses suggested a more complex heterogeneous pattern with a well-coping and a resigning subgroup of participants. Linguistic computational analyses uncovered that topics and n-gram frequencies shifted towards attention to the vaccination programme and away from general worrying. Implications for public mental health efforts in identifying people at heightened risk are discussed. The dataset is made publicly available.},
	language = {en},
	number = {1},
	urldate = {2021-12-03},
	journal = {Scientific Reports},
	author = {Mozes, Maximilian and van der Vegt, Isabelle and Kleinberg, Bennett},
	month = nov,
	year = {2021},
	note = {Bandiera\_abtest: a
Cc\_license\_type: cc\_by
Cg\_type: Nature Research Journals
Number: 1
Primary\_atype: Research
Publisher: Nature Publishing Group
Subject\_term: Human behaviour;Population screening
Subject\_term\_id: human-behaviour;population-screening},
	keywords = {Human behaviour, Population screening},
	pages = {23114},
	file = {Snapshot:/Users/bennettkleinberg/Zotero/storage/6B6EY4IH/s41598-021-02414-9.html:text/html},
}

@inproceedings{morris-etal-2020-textattack,
    title = "{T}ext{A}ttack: A Framework for Adversarial Attacks, Data Augmentation, and Adversarial Training in {NLP}",
    author = "Morris, John  and
      Lifland, Eli  and
      Yoo, Jin Yong  and
      Grigsby, Jake  and
      Jin, Di  and
      Qi, Yanjun",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations",
    month = oct,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.emnlp-demos.16",
    doi = "10.18653/v1/2020.emnlp-demos.16",
    pages = "119--126",
    abstract = "While there has been substantial research using adversarial attacks to analyze NLP models, each attack is implemented in its own code repository. It remains challenging to develop NLP attacks and utilize them to improve model performance. This paper introduces TextAttack, a Python framework for adversarial attacks, data augmentation, and adversarial training in NLP. TextAttack builds attacks from four components: a goal function, a set of constraints, a transformation, and a search method. TextAttack{'}s modular design enables researchers to easily construct attacks from combinations of novel and existing components. TextAttack provides implementations of 16 adversarial attacks from the literature and supports a variety of models and datasets, including BERT and other transformers, and all GLUE tasks. TextAttack also includes data augmentation and adversarial training modules for using components of adversarial attacks to improve model accuracy and robustness.TextAttack is democratizing NLP: anyone can try data augmentation and adversarial training on any model or dataset, with just a few lines of code. Code and tutorials are available at https://github.com/QData/TextAttack.",
}

@inproceedings{mozes-etal-2021-contrasting,
    title = "Contrasting Human- and Machine-Generated Word-Level Adversarial Examples for Text Classification",
    author = "Mozes, Maximilian  and
      Bartolo, Max  and
      Stenetorp, Pontus  and
      Kleinberg, Bennett  and
      Griffin, Lewis",
    booktitle = "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2021",
    address = "Online and Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.emnlp-main.651",
    doi = "10.18653/v1/2021.emnlp-main.651",
    pages = "8258--8270",
    abstract = "Research shows that natural language processing models are generally considered to be vulnerable to adversarial attacks; but recent work has drawn attention to the issue of validating these adversarial inputs against certain criteria (e.g., the preservation of semantics and grammaticality). Enforcing constraints to uphold such criteria may render attacks unsuccessful, raising the question of whether valid attacks are actually feasible. In this work, we investigate this through the lens of human language ability. We report on crowdsourcing studies in which we task humans with iteratively modifying words in an input text, while receiving immediate model feedback, with the aim of causing a sentiment classification model to misclassify the example. Our findings suggest that humans are capable of generating a substantial amount of adversarial examples using semantics-preserving word substitutions. We analyze how human-generated adversarial examples compare to the recently proposed TextFooler, Genetic, BAE and SememePSO attack algorithms on the dimensions naturalness, preservation of sentiment, grammaticality and substitution rate. Our findings suggest that human-generated adversarial examples are not more able than the best algorithms to generate natural-reading, sentiment-preserving examples, though they do so by being much more computationally efficient.",
}

@inproceedings{mozes-etal-2021-frequency,
    title = "Frequency-Guided Word Substitutions for Detecting Textual Adversarial Examples",
    author = "Mozes, Maximilian  and
      Stenetorp, Pontus  and
      Kleinberg, Bennett  and
      Griffin, Lewis",
    booktitle = "Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume",
    month = apr,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.eacl-main.13",
    doi = "10.18653/v1/2021.eacl-main.13",
    pages = "171--186",
    abstract = "Recent efforts have shown that neural text processing models are vulnerable to adversarial examples, but the nature of these examples is poorly understood. In this work, we show that adversarial attacks against CNN, LSTM and Transformer-based classification models perform word substitutions that are identifiable through frequency differences between replaced words and their corresponding substitutions. Based on these findings, we propose frequency-guided word substitutions (FGWS), a simple algorithm exploiting the frequency properties of adversarial word substitutions for the detection of adversarial examples. FGWS achieves strong performance by accurately detecting adversarial examples on the SST-2 and IMDb sentiment datasets, with F1 detection scores of up to 91.4{\%} against RoBERTa-based classification models. We compare our approach against a recently proposed perturbation discrimination framework and show that we outperform it by up to 13.0{\%} F1.",
}

@inproceedings{alzantot-etal-2018-generating,
    title = "Generating Natural Language Adversarial Examples",
    author = "Alzantot, Moustafa  and
      Sharma, Yash  and
      Elgohary, Ahmed  and
      Ho, Bo-Jhang  and
      Srivastava, Mani  and
      Chang, Kai-Wei",
    booktitle = "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
    month = oct # "-" # nov,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D18-1316",
    doi = "10.18653/v1/D18-1316",
    pages = "2890--2896",
    abstract = "Deep neural networks (DNNs) are vulnerable to adversarial examples, perturbations to correctly classified examples which can cause the model to misclassify. In the image domain, these perturbations can often be made virtually indistinguishable to human perception, causing humans and state-of-the-art models to disagree. However, in the natural language domain, small perturbations are clearly perceptible, and the replacement of a single word can drastically alter the semantics of the document. Given these challenges, we use a black-box population-based optimization algorithm to generate semantically and syntactically similar adversarial examples that fool well-trained sentiment analysis and textual entailment models with success rates of 97{\%} and 70{\%}, respectively. We additionally demonstrate that 92.3{\%} of the successful sentiment analysis adversarial examples are classified to their original label by 20 human annotators, and that the examples are perceptibly quite similar. Finally, we discuss an attempt to use adversarial training as a defense, but fail to yield improvement, demonstrating the strength and diversity of our adversarial examples. We hope our findings encourage researchers to pursue improving the robustness of DNNs in the natural language domain.",
}

@article{hofman2021integrating,
  title={Integrating explanation and prediction in computational social science},
  author={Hofman, Jake M and Watts, Duncan J and Athey, Susan and Garip, Filiz and Griffiths, Thomas L and Kleinberg, Jon and Margetts, Helen and Mullainathan, Sendhil and Salganik, Matthew J and Vazire, Simine and others},
  journal={Nature},
  volume={595},
  number={7866},
  pages={181--188},
  year={2021},
  publisher={Nature Publishing Group}
}

@article{jackson1986mary,
  title={What Mary didn't know},
  author={Jackson, Frank},
  journal={The Journal of Philosophy},
  volume={83},
  number={5},
  pages={291--295},
  year={1986},
  publisher={JSTOR}
}

@article{jackson1982epiphenomenal,
  title={Epiphenomenal qualia},
  author={Jackson, Frank},
  journal={The Philosophical Quarterly (1950-)},
  volume={32},
  number={127},
  pages={127--136},
  year={1982},
  publisher={JSTOR}
}

@article{lazer2020computational,
  title={Computational social science: Obstacles and opportunities},
  author={Lazer, David MJ and Pentland, Alex and Watts, Duncan J and Aral, Sinan and Athey, Susan and Contractor, Noshir and Freelon, Deen and Gonzalez-Bailon, Sandra and King, Gary and Margetts, Helen and others},
  journal={Science},
  volume={369},
  number={6507},
  pages={1060--1062},
  year={2020},
  publisher={American Association for the Advancement of Science}
}

@inproceedings{GoodfellowSS14,
  author    = {Ian J. Goodfellow and
               Jonathon Shlens and
               Christian Szegedy},
  editor    = {Yoshua Bengio and
               Yann LeCun},
  title     = {Explaining and Harnessing Adversarial Examples},
  booktitle = {3rd International Conference on Learning Representations, {ICLR} 2015,
               San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings},
  year      = {2015},
  url       = {http://arxiv.org/abs/1412.6572},
  timestamp = {Thu, 25 Jul 2019 14:25:38 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/GoodfellowSS14.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{morris-etal-2020-reevaluating,
    title = "Reevaluating Adversarial Examples in Natural Language",
    author = "Morris, John  and
      Lifland, Eli  and
      Lanchantin, Jack  and
      Ji, Yangfeng  and
      Qi, Yanjun",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2020",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.findings-emnlp.341",
    doi = "10.18653/v1/2020.findings-emnlp.341",
    pages = "3829--3839",
    abstract = "State-of-the-art attacks on NLP models lack a shared definition of a what constitutes a successful attack. We distill ideas from past work into a unified framework: a successful natural language adversarial example is a perturbation that fools the model and follows some linguistic constraints. We then analyze the outputs of two state-of-the-art synonym substitution attacks. We find that their perturbations often do not preserve semantics, and 38{\%} introduce grammatical errors. Human surveys reveal that to successfully preserve semantics, we need to significantly increase the minimum cosine similarities between the embeddings of swapped words and between the sentence encodings of original and perturbed sentences.With constraints adjusted to better preserve semantics and grammaticality, the attack success rate drops by over 70 percentage points.",
}


@article{van_der_vegt_multi-year_2022,
	title = {A multi-year study on insights into emotional coping during the pandemic},
	author = {van der Vegt, Isabelle and Mozes, Maximilian and Kleinberg, Bennett},
	year = {2022},
}

@inproceedings{NEURIPS2020_1457c0d6,
 author = {Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel and Wu, Jeffrey and Winter, Clemens and Hesse, Chris and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
 pages = {1877--1901},
 publisher = {Curran Associates, Inc.},
 title = {Language Models are Few-Shot Learners},
 url = {https://proceedings.neurips.cc/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf},
 volume = {33},
 year = {2020}
}


@inproceedings{cer_universal_2018,
	address = {Brussels, Belgium},
	title = {Universal {Sentence} {Encoder} for {English}},
	url = {http://aclweb.org/anthology/D18-2029},
	doi = {10.18653/v1/D18-2029},
	abstract = {We present easy-to-use TensorFlow Hub sentence embedding models having good task transfer performance. Model variants allow for trade-offs between accuracy and compute resources. We report the relationship between model complexity, resources, and transfer performance. Comparisons are made with baselines without transfer learning and to baselines that incorporate word-level transfer. Transfer learning using sentence-level embeddings is shown to outperform models without transfer learning and often those that use only word-level transfer. We show good transfer task performance with minimal training data and obtain encouraging results on word embedding association tests (WEAT) of model bias.},
	language = {en},
	urldate = {2021-04-11},
	booktitle = {Proceedings of the 2018 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}: {System} {Demonstrations}},
	publisher = {Association for Computational Linguistics},
	author = {Cer, Daniel and Yang, Yinfei and Kong, Sheng-yi and Hua, Nan and Limtiaco, Nicole and St. John, Rhomni and Constant, Noah and Guajardo-Cespedes, Mario and Yuan, Steve and Tar, Chris and Strope, Brian and Kurzweil, Ray},
	year = {2018},
	pages = {169--174},
	file = {Cer et al. - 2018 - Universal Sentence Encoder for English.pdf:/Users/bennettkleinberg/Zotero/storage/BKY8E9U6/Cer et al. - 2018 - Universal Sentence Encoder for English.pdf:application/pdf},
}


@article{miotto_who_2022,
	title = {Who is {GPT}-3?},
	author = {Miotto, Marilu and Rossberg, Nicola and Kleinberg, Bennett},
	year = {(ongoing)},
}


@misc{stevenson_putting_2022,
	title = {Putting {GPT}-3's {Creativity} to the ({Alternative} {Uses}) {Test}},
	url = {http://arxiv.org/abs/2206.08932},
	doi = {10.48550/arXiv.2206.08932},
	abstract = {AI large language models have (co-)produced amazing written works from newspaper articles to novels and poetry. These works meet the standards of the standard definition of creativity: being original and useful, and sometimes even the additional element of surprise. But can a large language model designed to predict the next text fragment provide creative, out-of-the-box, responses that still solve the problem at hand? We put Open AI's generative natural language model, GPT-3, to the test. Can it provide creative solutions to one of the most commonly used tests in creativity research? We assessed GPT-3's creativity on Guilford's Alternative Uses Test and compared its performance to previously collected human responses on expert ratings of originality, usefulness and surprise of responses, flexibility of each set of ideas as well as an automated method to measure creativity based on the semantic distance between a response and the AUT object in question. Our results show that -- on the whole -- humans currently outperform GPT-3 when it comes to creative output. But, we believe it is only a matter of time before GPT-3 catches up on this particular task. We discuss what this work reveals about human and AI creativity, creativity testing and our definition of creativity.},
	urldate = {2022-08-24},
	publisher = {arXiv},
	author = {Stevenson, Claire and Smal, Iris and Baas, Matthijs and Grasman, Raoul and van der Maas, Han},
	month = jun,
	year = {2022},
	note = {arXiv:2206.08932 [cs]},
	keywords = {Computer Science - Human-Computer Interaction, Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
	file = {arXiv.org Snapshot:/Users/bennettkleinberg/Zotero/storage/PI9QHXNM/2206.html:text/html},
}

@article{binz_using_2022,
	title = {Using cognitive psychology to understand {GPT}-3},
	copyright = {Creative Commons Attribution 4.0 International},
	url = {https://arxiv.org/abs/2206.14576},
	doi = {10.48550/ARXIV.2206.14576},
	abstract = {We study GPT-3, a recent large language model, using tools from cognitive psychology. More specifically, we assess GPT-3's decision-making, information search, deliberation, and causal reasoning abilities on a battery of canonical experiments from the literature. We find that much of GPT-3's behavior is impressive: it solves vignette-based tasks similarly or better than human subjects, is able to make decent decisions from descriptions, outperforms humans in a multi-armed bandit task, and shows signatures of model-based reinforcement learning. Yet we also find that small perturbations to vignette-based tasks can lead GPT-3 vastly astray, that it shows no signatures of directed exploration, and that it fails miserably in a causal reasoning task. These results enrich our understanding of current large language models and pave the way for future investigations using tools from cognitive psychology to study increasingly capable and opaque artificial agents.},
	urldate = {2022-09-18},
	author = {Binz, Marcel and Schulz, Eric},
	year = {2022},
	note = {Publisher: arXiv
Version Number: 1},
	keywords = {Artificial Intelligence (cs.AI), Computation and Language (cs.CL), FOS: Computer and information sciences, Machine Learning (cs.LG)},
}

@misc{weber_does_2021,
	title = {Does {GPT}-3 have a theory of mind?},
	url = {https://medium.com/@julian78w/does-gpt-3-have-a-theory-of-mind-890fb0c7bf48},
	abstract = {Introduction},
	language = {en},
	urldate = {2022-09-18},
	journal = {Medium},
	author = {Weber, Julian},
	month = oct,
	year = {2021},
	file = {Snapshot:/Users/bennettkleinberg/Zotero/storage/RUCUNPYR/does-gpt-3-have-a-theory-of-mind-890fb0c7bf48.html:text/html},
}

@article{van_der_maas_how_2021,
	title = {How much intelligence is there in artificial intelligence? {A} 2020 update},
	volume = {87},
	issn = {01602896},
	shorttitle = {How much intelligence is there in artificial intelligence?},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0160289621000325},
	doi = {10.1016/j.intell.2021.101548},
	language = {en},
	urldate = {2022-09-18},
	journal = {Intelligence},
	author = {van der Maas, Han L.J. and Snoek, Lukas and Stevenson, Claire E.},
	month = jul,
	year = {2021},
	pages = {101548},
}


@article{kleinberg_netanos-named_2017,
	title = {{NETANOS}-{Named} entity-based {Text} {Anonymization} for {Open} {Science}},
	url = {https://osf.io/preprints/w9nhb/},
	urldate = {2017-07-22},
	journal = {OSF Preprint},
	author = {Kleinberg, Bennett and Mozes, Maximilian and van der Toolen, Yaloe and Verschuere, Bruno},
	year = {2017},
	file = {[PDF] osf.io:/Users/bennettkleinberg/Zotero/storage/XVJ39PCI/Kleinberg et al. - 2017 - NETANOS-Named entity-based Text Anonymization for .pdf:application/pdf;Snapshot:/Users/bennettkleinberg/Zotero/storage/II35WF2W/w9nhb.html:text/html},
}

@article{kleinberg_textwash_2022,
	title = {Textwash - automated open-source text anonymisation},
	url = {http://arxiv.org/abs/2208.13081},
	doi = {10.48550/arXiv.2208.13081},
	abstract = {The increased use of text data in social science research has benefited from easy-to-access data (e.g., Twitter). That trend comes at the cost of research requiring sensitive but hard-to-share data (e.g., interview data, police reports, electronic health records). We introduce a solution to that stalemate with the open-source text anonymisation software\_Textwash\_. This paper presents the empirical evaluation of the tool using the TILD criteria: a technical evaluation (how accurate is the tool?), an information loss evaluation (how much information is lost in the anonymisation process?) and a de-anonymisation test (can humans identify individuals from anonymised text data?). The findings suggest that Textwash performs similar to state-of-the-art entity recognition models and introduces a negligible information loss of 0.84\%. For the de-anonymisation test, we tasked humans to identify individuals by name from a dataset of crowdsourced person descriptions of very famous, semi-famous and non-existing individuals. The de-anonymisation rate ranged from 1.01-2.01\% for the realistic use cases of the tool. We replicated the findings in a second study and concluded that Textwash succeeds in removing potentially sensitive information that renders detailed person descriptions practically anonymous.},
	urldate = {2022-09-13},
	journal = {arXiv:2208.13081},
	author = {Kleinberg, Bennett and Davies, Toby and Mozes, Maximilian},
	month = aug,
	year = {2022},
	note = {arXiv:2208.13081 [cs]
version: 1},
	keywords = {Computer Science - Computers and Society, Computer Science - Computation and Language},
	file = {arXiv.org Snapshot:/Users/bennettkleinberg/Zotero/storage/LLU26FUN/2208.html:text/html},
}


@article{mozes_no_2021,
	title = {No {Intruder}, no {Validity}: {Evaluation} {Criteria} for {Privacy}-{Preserving} {Text} {Anonymization}},
	shorttitle = {No {Intruder}, no {Validity}},
	url = {http://arxiv.org/abs/2103.09263},
	abstract = {For sensitive text data to be shared among NLP researchers and practitioners, shared documents need to comply with data protection and privacy laws. There is hence a growing interest in automated approaches for text anonymization. However, measuring such methods' performance is challenging: missing a single identifying attribute can reveal an individual's identity. In this paper, we draw attention to this problem and argue that researchers and practitioners developing automated text anonymization systems should carefully assess whether their evaluation methods truly reflect the system's ability to protect individuals from being re-identified. We then propose TILD, a set of evaluation criteria that comprises an anonymization method's technical performance, the information loss resulting from its anonymization, and the human ability to de-anonymize redacted documents. These criteria may facilitate progress towards a standardized way for measuring anonymization performance.},
	urldate = {2021-03-18},
	journal = {arXiv:2103.09263 [cs]},
	author = {Mozes, Maximilian and Kleinberg, Bennett},
	month = mar,
	year = {2021},
	note = {arXiv: 2103.09263},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv.org Snapshot:/Users/bennettkleinberg/Zotero/storage/BX6RQ5W6/2103.html:text/html},
}
